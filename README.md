# üè• Projeto de Engenharia de Dados na Sa√∫de no Brasil

Desenvolvimento de uma pipeline de dados para ingest√£o e transforma√ß√£o de grandes volumes de dados de interna√ß√µes hospitalares (AIH - Autoriza√ß√£o de Interna√ß√£o Hospitalar) do DATASUS.

## üõ†Ô∏è Componentes e Ferramentas:
- **AWS S3:** Armazenamento em camadas (Bronze, Silver e Gold).
- **Apache Spark:** Processamento distribu√≠do de dados.
- **Airflow:** Orquestra√ß√£o das tarefas da pipeline.
- **Docker:** Cria√ß√£o de containers para padronizar ambientes.
- **SQL:** Para consultas e agrega√ß√µes durante a transforma√ß√£o.

## Url Download

https://datasus.saude.gov.br/transferencia-de-arquivos/#

![alt text](imgs/tranfer-arquivos-sih-datasus.jpg)

## Escopo Inicial para AIH

**1. Definir o Per√≠odo de An√°lise:**

- Intervalo de 3 a 5 anos (2018 a 2023).
- Dados limitados pelo estado de S√£o Paulo para reduzir o volume inicial.

**2. Vari√°veis de Interesse:**

- Diagn√≥stico principal (CID).
- Procedimentos realizados.
- Dura√ß√£o da interna√ß√£o.
- Custos da interna√ß√£o.
- Tipo de hospital (p√∫blico ou privado).

**3. Objetivos Anal√≠ticos Focados:**

- Identificar os diagn√≥sticos mais frequentes por tipo de hospital e regi√£o.
- Analisar os custos m√©dios por tipo de interna√ß√£o.
- Avaliar a evolu√ß√£o do tempo m√©dio de interna√ß√£o para doen√ßas cr√¥nicas.

**4. Dados Complementares (Opcional):**

- Tabelas de refer√™ncias do DATASUS, como c√≥digos CID e procedimentos.

## Primeiras Etapas

**Ingest√£o:**

- Baixar os dados da AIH do DATASUS (formato CSV ou DBF).
  - (https://datasus.saude.gov.br/transferencia-de-arquivos/#)
- Ingest√£o inicial para AWS S3 (camada Bronze).

**Processamento:**

- Padronizar e limpar os dados no Apache Spark.
- Estruturar tabelas anal√≠ticas para facilitar consultas.

**Resultados Simples:**

- Criar uma tabela b√°sica com os diagn√≥sticos mais frequentes e custos associados.

## Fase 1: Planejamento da Pipeline de Dados

### Etapas

**1. Estrutura da Pipeline:**

- Camada de Ingest√£o:
  - Baixar os dados de AIH do DATASUS para a camada "Bronze" do AWS S3.
- Camada de Processamento:
  - Limpar, padronizar e transformar os dados utilizando Apache Spark.
- Camada de An√°lise:
  - Criar tabelas anal√≠ticas (camada "Gold") prontas para consumo por ferramentas de visualiza√ß√£o e an√°lises.

**2. Componentes e Ferramentas:**

- AWS S3: Armazenamento em camadas (Bronze, Silver e Gold).
- Apache Spark: Processamento distribu√≠do de dados.
- Airflow: Orquestra√ß√£o das tarefas da pipeline.
- Docker: Cria√ß√£o de containers para padronizar ambientes.
- SQL: Para consultas e agrega√ß√µes durante a transforma√ß√£o.

**3. Vari√°veis Inicialmente Selecionadas:**

- Diagn√≥sticos (CID principal e secund√°rio).
- Tempo de interna√ß√£o.
- Custo total da interna√ß√£o.
- Tipo de hospital (p√∫blico ou privado).
- Data de admiss√£o e alta.

## Fase 2: Elabora√ß√£o de Scripts Iniciais

**1. Script para Ingest√£o de Dados:**

- O script buscar√° arquivos CSV dispon√≠veis no DATASUS e os carregar√° para a camada Bronze do AWS S3.
- C√≥digo Python para Ingest√£o: [scripts/ingestion.py](scripts/ingestion.py)

**2. Script para Transforma√ß√£o (Camada Silver):**

- Ap√≥s a ingest√£o, o pr√≥ximo passo ser√° usar o Apache Spark para limpar e padronizar os dados.
- Tarefa Inicial com Spark
- Objetivo: Filtrar colunas principais, remover registros inv√°lidos e salvar como Parquet.
- C√≥digo Python para Transforma√ß√£o: [scripts/transformation.py](scripts/transformation.py)

## Especifica√ß√£o T√©cnica

- **IDE:** VSCode (Visual Studio Code)
- **Linguagem:** Python
  - **Bibliotecas (principais):** boto3, pyspark

## Configura√ß√£o e Execu√ß√£o

### 1. Criar o Ambiente Virtual

- **1.1. Abra o terminal ou o PowerShell no Windows.**

- **1.2. Navegue at√© o diret√≥rio base do projeto:**
  cd D:\GitHub\ds-aih-pipeline

- **1.3. Crie o ambiente virtual:**
  python -m venv venv

- **1.4. Ative o ambiente virtual:**
  venv\Scripts\activate

### 2. Instalar as Depend√™ncias

- **2.1. Instale as depend√™ncias do arquivo requirements.txt existente**
  pip install -r requirements.txt

- **2.2. OU CASO exclua o arquivo requirements.txt existente:**

  - **2.2.1. Primeiro: Instale as depend√™ncias necess√°rias com pip:**
    pip install pyspark
    pip install boto3

  - **2.2.2. Segundo: Gere arquivo requirements.txt**
    pip freeze > requirements.txt

### 3. Executar o Script

- **3.1. Navegue at√© o diret√≥rio onde est√° o script:**
  cd D:\GitHub\ds-aih-pipeline\scripts

- **3.2. Execute o script de ingestion com o Python:**
  python ingestion.py

### 4. Manuten√ß√£o do Ambiente

- **4.1. Para Desativar o Ambiente:**
  **Quando terminar de usar o ambiente virtual, voc√™ pode desativ√°-lo com o comando:**
  deactivate

- **4.2. Para Reativar o Ambiente:**
  **Sempre que quiser executar novamente, reative o ambiente com:**
  venv\Scripts\activate
