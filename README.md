# üè• Projeto de Engenharia de Dados na Sa√∫de no Brasil

Desenvolvimento de uma pipeline de dados para ingest√£o e transforma√ß√£o de grandes volumes de dados de interna√ß√µes hospitalares (AIH - Autoriza√ß√£o de Interna√ß√£o Hospitalar) do DATASUS.

## üõ†Ô∏è Componentes e Ferramentas:

- AWS S3: Armazenamento em camadas (Bronze, Silver e Gold).
- Apache Spark: Processamento distribu√≠do de dados.
- Airflow: Orquestra√ß√£o das tarefas da pipeline.
- Docker: Cria√ß√£o de containers para padronizar ambientes.
- SQL: Para consultas e agrega√ß√µes durante a transforma√ß√£o.

## Url Download

https://datasus.saude.gov.br/transferencia-de-arquivos/#

![alt text](imgs/tranfer-arquivos-sih-datasus.jpg)

## Escopo Inicial para AIH

1. Definir o Per√≠odo de An√°lise:

- Intervalo de 3 a 5 anos (2018 a 2023).
- Dados limitados pelo estado de S√£o Paulo para reduzir o volume inicial.

2. Vari√°veis de Interesse:

- Diagn√≥stico principal (CID).
- Procedimentos realizados.
- Dura√ß√£o da interna√ß√£o.
- Custos da interna√ß√£o.
- Tipo de hospital (p√∫blico ou privado).

3. Objetivos Anal√≠ticos Focados:

- Identificar os diagn√≥sticos mais frequentes por tipo de hospital e regi√£o.
- Analisar os custos m√©dios por tipo de interna√ß√£o.
- Avaliar a evolu√ß√£o do tempo m√©dio de interna√ß√£o para doen√ßas cr√¥nicas.

4. Dados Complementares (Opcional):

- Tabelas de refer√™ncias do DATASUS, como c√≥digos CID e procedimentos.

## Primeiras Etapas

Ingest√£o:

- Baixar os dados da AIH do DATASUS (formato CSV ou DBF).
  - https://datasus.saude.gov.br/transferencia-de-arquivos/#
- Ingest√£o inicial para AWS S3 (camada Bronze).

Processamento:

- Padronizar e limpar os dados no Apache Spark.
- Estruturar tabelas anal√≠ticas para facilitar consultas.

Resultados Simples:

- Criar uma tabela b√°sica com os diagn√≥sticos mais frequentes e custos associados.

## Fase 1: Planejamento da Pipeline de Dados

### Etapas

1. Estrutura da Pipeline:

- Camada de Ingest√£o:
  - Baixar os dados de AIH do DATASUS para a camada "Bronze" do AWS S3.
- Camada de Processamento:
  - Limpar, padronizar e transformar os dados utilizando Apache Spark.
- Camada de An√°lise:
  - Criar tabelas anal√≠ticas (camada "Gold") prontas para consumo por ferramentas de visualiza√ß√£o e an√°lises.

2. Componentes e Ferramentas:

- AWS S3: Armazenamento em camadas (Bronze, Silver e Gold).
- Apache Spark: Processamento distribu√≠do de dados.
- Airflow: Orquestra√ß√£o das tarefas da pipeline.
- Docker: Cria√ß√£o de containers para padronizar ambientes.
- SQL: Para consultas e agrega√ß√µes durante a transforma√ß√£o.

3. Vari√°veis Inicialmente Selecionadas:

- Diagn√≥sticos (CID principal e secund√°rio).
- Tempo de interna√ß√£o.
- Custo total da interna√ß√£o.
- Tipo de hospital (p√∫blico ou privado).
- Data de admiss√£o e alta.

4: Orquestra√ß√£o no Airflow

- Automatiza√ß√£o: Orquestrar os processos de ingest√£o e transforma√ß√£o para serem executados de forma programada.
- Depend√™ncias: Gerenciar a sequ√™ncia correta das etapas, garantindo que a transforma√ß√£o s√≥ ocorra ap√≥s a ingest√£o.
- Escalabilidade: Preparar para futuras etapas, como enriquecimento, valida√ß√£o ou carga em bancos de dados.
- Monitoramento: Identificar rapidamente falhas nas etapas e reprocessar apenas o necess√°rio.

## Fase 2: Elabora√ß√£o de Scripts Iniciais

1. Script para Ingest√£o de Dados:

- O script buscar√° arquivos CSV dispon√≠veis no DATASUS e os carregar√° para a camada Bronze do AWS S3.
- C√≥digo Python para Ingest√£o: [scripts/ingestion.py](scripts/ingestion.py)

2. Script para Transforma√ß√£o (Camada Silver):

- Ap√≥s a ingest√£o, o pr√≥ximo passo ser√° usar o Apache Spark para limpar e padronizar os dados.
- Tarefa Inicial com Spark
- Objetivo: Filtrar colunas principais, remover registros inv√°lidos e salvar como Parquet.
- C√≥digo Python para Transforma√ß√£o: [scripts/transformation.py](scripts/transformation.py)

## Fase 3: Orquestra√ß√£o no Airflow

1. Configura√ß√£o do Ambiente do Airflow:

- Usaremos o Amazon MWAA (Managed Workflows for Apache Airflow) como a vers√£o gerenciada do Airflow na AWS, eliminando a necessidade de configur√°-lo manualmente.
- Instalar Airflow Localmente com Docker:
  - Criar o Arquivo docker-compose.yaml.
  - Subir o Airflow: Executar os comandos no terminal:

```bash
## bash
mkdir dags logs plugins
docker-compose up -d
```

- Acessar a Interface:
  - Abra o navegador e v√° para http://localhost:8080.
  - Use as credenciais padr√£o:
    - Usu√°rio: `airflow`
    - Senha: `airflow`

2. Estrutura da DAG:

- Tarefa 1: Baixar dados do DATASUS (ingest√£o para o S3).
- Tarefa 2: Processar os dados no Spark (transforma√ß√£o para S3 Silver).
- Tarefa 3: (opcional): Validar os dados e enviar um alerta em caso de falha.

3. Monitoramento:

- Configurar o monitoramento b√°sico com alertas via e-mail ou Amazon SNS.
- Incluir logs detalhados para cada tarefa no Airflow, facilitando o diagn√≥stico de erros.

4. Integra√ß√£o com o AWS:

- Configurar a conex√£o do Airflow com:
  - S3: Para leitura/escrita de dados.
  - EKS (Kubernetes): Para rodar tarefas Spark (opcional no in√≠cio).
  - CloudWatch: Para logs centralizados.

---

## Especifica√ß√£o T√©cnica

- IDE: VSCode (Visual Studio Code)
- Linguagem: Python
  - Bibliotecas (principais): boto3, pyspark
- OS: Windows 11

## üöÄ Configura√ß√£o e Execu√ß√£o

### 1. Criar o Ambiente Virtual

1.1. Abra o terminal ou o PowerShell no Windows.

1.2. Navegue at√© o diret√≥rio base do projeto:

  `cd D:\GitHub\ds-aih-pipeline`

1.3. Crie o ambiente virtual:

  `python -m venv airflow_env`

1.4. Ative o ambiente virtual:

  `venv\Scripts\activate`

### 2. Criar e Ativar um Ambiente Virtual antes de Instalar as Depend√™ncias

2.1. Criar e Ativar Ambiente Virtual:
  - A vers√£o mais recente do Airflow geralmente exige um ambiente virtual dedicado para evitar conflitos de depend√™ncias.
  - √â uma boa pr√°tica criar e ativar um ambiente virtual antes de instalar:

  `python -m venv airflow_env`
    
  `source airflow_env/bin/activate`
    
  `airflow_env\Scripts\activate`
    
2.2. Instalar as Depend√™ncias do arquivo requirements.txt existente:  

  `pip install -r requirements.txt`

2.3. OU N√ÉO EXISTE o arquivo requirements.txt:

  2.3.1. Primeiro: Instale as depend√™ncias necess√°rias com pip:    

  ```bash
  pip install pyspark  
  pip install boto3  
  pip install apache-airflow  
  pip install apache-airflow[aws]
  ```  
      
  Instalando Extras e Operadores Adicionais:
    - Se voc√™ precisar de operadores adicionais, como para integra√ß√£o com o AWS, ajuste o comando para incluir o extra [aws].
    - O extra [aws] inclui bibliotecas √∫teis para conectar ao S3 ou outros servi√ßos da AWS.
  
  2.3.2. Segundo: Gere arquivo requirements.txt
  
  `pip freeze > requirements.txt`

### 3. Executar o Script

  3.1. Navegue at√© o diret√≥rio onde est√° o script:
  
  `cd D:\GitHub\ds-aih-pipeline\scripts`

  3.2. Execute o script de ingestion com o Python:

  `python ingestion.py`

### 4. Manuten√ß√£o do Ambiente

  4.1. Para Desativar o Ambiente:
  Quando terminar de usar o ambiente virtual, voc√™ pode desativ√°-lo com o comando:
  
  `deactivate`

  4.2. Para Reativar o Ambiente:
  Sempre que quiser executar novamente, reative o ambiente com:
  
  `venv\Scripts\activate`                           
