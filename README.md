# ğŸ¥ Projeto de Engenharia de Dados na SaÃºde no Brasil

Desenvolvimento de uma pipeline de dados para ingestÃ£o e transformaÃ§Ã£o de grandes volumes de dados de internaÃ§Ãµes hospitalares (AIH - AutorizaÃ§Ã£o de InternaÃ§Ã£o Hospitalar) do DATASUS.
- Leia sobre o [Escopo do Projeto](projeto.md#escopo-do-projeto-de-engenharia-de-dados-na-saÃºde-no-brasil)

### Ajustes e melhorias

O projeto ainda estÃ¡ em desenvolvimento e as prÃ³ximas atualizaÃ§Ãµes serÃ£o voltadas para as seguintes tarefas:

- [x] Tarefa 1
- [x] Tarefa 2
- [x] Tarefa 3
- [ ] Tarefa 4
- [ ] Tarefa 5
---

- [ğŸ’» EspecificaÃ§Ã£o TÃ©cnica](#-especificaÃ§Ã£o-tÃ©cnica)
- [ğŸ› ï¸ Componentes e Ferramentas](#%EF%B8%8F-componentes-e-ferramentas)
- [ğŸš€ ConfiguraÃ§Ã£o e ExecuÃ§Ã£o](#-configuraÃ§Ã£o-e-execuÃ§Ã£o)
    - [1. Criar o Ambiente Virtual](#1-criar-o-ambiente-virtual)
    - [2. Instalar as DependÃªncias](#2-instalar-as-dependÃªncias)
    - [3. Executar o Script](#3-executar-o-script)
    - [4. ManutenÃ§Ã£o do Ambiente](#4-manutenÃ§Ã£o-do-ambiente)
- [ğŸ“‘ Url Download](#-url-download)

---

## ğŸ’» EspecificaÃ§Ã£o TÃ©cnica

- IDE: VSCode (Visual Studio Code)
- Linguagem: Python
  - Bibliotecas (principais): boto3, pyspark
- OS: Windows 11

## ğŸ› ï¸ Componentes e Ferramentas

- AWS S3: Armazenamento em camadas (Bronze, Silver e Gold).
- Apache Spark: Processamento distribuÃ­do de dados.
- Airflow: OrquestraÃ§Ã£o das tarefas da pipeline.
- Docker: CriaÃ§Ã£o de containers para padronizar ambientes.
- SQL: Para consultas e agregaÃ§Ãµes durante a transformaÃ§Ã£o.

## ğŸš€ ConfiguraÃ§Ã£o e ExecuÃ§Ã£o

### 1. Criar o Ambiente Virtual

1.1. Abra o terminal ou o PowerShell no Windows.

1.2. Navegue atÃ© o diretÃ³rio base do projeto:

`cd D:\GitHub\ds-aih-pipeline`

1.3. Crie o ambiente virtual:

`python -m venv venv`

1.4. Ative o ambiente virtual:

`venv\Scripts\activate`

### 2. Instalar as DependÃªncias

2.1. Instalar as DependÃªncias do arquivo requirements.txt existente:

`pip install -r requirements.txt`

2.2. OU NÃƒO EXISTE o arquivo requirements.txt:

2.2.1. Primeiro: Instale as dependÃªncias necessÃ¡rias com pip:

Instalando Extras e Operadores Adicionais: - Se vocÃª precisar de operadores adicionais, como para integraÃ§Ã£o com o AWS, ajuste o comando para incluir o extra [aws]. - O extra [aws] inclui bibliotecas Ãºteis para conectar ao S3 ou outros serviÃ§os da AWS.

```bash
# por uma Ãºnica linha
pip install pandas pyspark boto3 apache-airflow apache-airflow[aws]
```

```bash
# linha por linha
pip install pandas
pip install pyspark
pip install boto3
pip install apache-airflow
pip install apache-airflow[aws]
```

2.2.2. Segundo: Gere arquivo requirements.txt

`pip freeze > requirements.txt`

### 3. Executar o Script

3.1. Navegue atÃ© o diretÃ³rio onde estÃ¡ o script:

`cd D:\GitHub\ds-aih-pipeline\scripts`

3.2. Execute o script de ingestion com o Python:

`python ingestion.py`

### 4. ManutenÃ§Ã£o do Ambiente

4.1. Para Desativar o Ambiente:
Quando terminar de usar o ambiente virtual, vocÃª pode desativÃ¡-lo com o comando:

`deactivate`

4.2. Para Reativar o Ambiente:
Sempre que quiser executar novamente, reative o ambiente com:

`venv\Scripts\activate`

## ğŸ“‘ Url Download

https://datasus.saude.gov.br/transferencia-de-arquivos/#

![alt text](imgs/tranfer-arquivos-sih-datasus.jpg)

